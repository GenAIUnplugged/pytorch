{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GenAIUnplugged/pytorch/blob/main/TransferLearning_Flowers102.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from torchvision.transforms import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torchvision.models import resnet50,ResNet50_Weights\n",
        "from tqdm import tqdm\n",
        "import torch.optim.lr_scheduler as lr_scheduler # Import scheduler\n",
        "\n",
        "# %%\n",
        "# Define transformations\n",
        "# Added Data Augmentation for training\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224), # Randomly crop and resize\n",
        "    transforms.RandomHorizontalFlip(), # Randomly flip horizontally\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Normalize with ImageNet stats\n",
        "])\n",
        "\n",
        "# No augmentation for validation or testing, only resize and normalize\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize(256), # Resize first\n",
        "    transforms.CenterCrop(224), # Then crop the center\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "# %%\n",
        "# Load Datasets (using separate train, validation, and test sets for Flowers102)\n",
        "train_ds = torchvision.datasets.Flowers102(\n",
        "    root=\"./data\",\n",
        "    split=\"train\",\n",
        "    transform=train_transform, # Use train_transform\n",
        "    download=True\n",
        ")\n",
        "\n",
        "val_ds = torchvision.datasets.Flowers102( # Load the validation set\n",
        "    root=\"./data\",\n",
        "    split=\"val\", # Specify validation split\n",
        "    transform=eval_transform, # Use eval_transform\n",
        "    download=True\n",
        ")\n",
        "\n",
        "test_ds = torchvision.datasets.Flowers102( # Load the test set\n",
        "    root=\"./data\",\n",
        "    split=\"test\", # Specify test split\n",
        "    transform=eval_transform, # Use eval_transform\n",
        "    download=True\n",
        ")\n",
        "\n",
        "# %%\n",
        "# Data Loaders\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_ds, shuffle=True, batch_size=32, num_workers=2) # Added num_workers\n",
        "val_loader = torch.utils.data.DataLoader(dataset=val_ds, shuffle=False, batch_size=32, num_workers=2) # Added num_workers and shuffle=False for consistent evaluation\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_ds, shuffle=False, batch_size=32, num_workers=2) # Added num_workers and shuffle=False for consistent evaluation\n",
        "\n",
        "\n",
        "# %%\n",
        "# Device configuration\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\") # Print device being used\n",
        "\n",
        "# %%\n",
        "# Load pre-trained ResNet50 model\n",
        "model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1) # Specify weights version\n",
        "model = model.to(device)\n",
        "\n",
        "# %%\n",
        "# Modify the final fully connected layer for 102 classes\n",
        "num_classes = 102 # Set number of classes based on your dataset (Flowers102 has 102)\n",
        "model.fc = nn.Linear(in_features=model.fc.in_features, out_features=num_classes)\n",
        "model = model.to(device)\n",
        "\n",
        "# %%\n",
        "# Define Loss function and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # Adam optimizer\n",
        "# Add a learning rate scheduler\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1) # Decrease learning rate by 10% every 30 epochs\n",
        "\n",
        "# %%\n",
        "# Training loop\n",
        "epochs = 50 # Increased number of epochs\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train() # Set model to training mode\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n",
        "\n",
        "    for batch, (image, label) in enumerate(progress_bar):\n",
        "        image, label = image.to(device), label.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        logits = model(image)\n",
        "        loss = criterion(logits, label)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        pred_probs = torch.softmax(input=logits, dim=1).argmax(dim=1)\n",
        "        correct = (pred_probs == label).sum().item()\n",
        "        total_correct += correct\n",
        "        total_loss += loss.item()\n",
        "        total_samples += label.size(0)\n",
        "\n",
        "        # Update progress bar\n",
        "        progress_bar.set_postfix(loss=total_loss / (batch + 1), accuracy=total_correct / total_samples)\n",
        "\n",
        "    # Step the scheduler\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f\"\\nEpoch {epoch+1}/{epochs} [Train] - Loss: {total_loss / len(train_loader):.4f}, Accuracy: {total_correct / total_samples:.4f}\")\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    progress_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Validation]\") # Use val_loader\n",
        "\n",
        "    with torch.no_grad(): # Disable gradient calculation for evaluation\n",
        "        for batch, (image, label) in enumerate(progress_bar):\n",
        "            image, label = image.to(device), label.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            val_logits = model(image)\n",
        "            val_loss = criterion(val_logits, label)\n",
        "\n",
        "            # Calculate accuracy\n",
        "            pred_probs = torch.softmax(input=val_logits, dim=1).argmax(dim=1)\n",
        "            correct = (pred_probs == label).sum().item()\n",
        "            total_correct += correct\n",
        "            total_loss += val_loss.item()\n",
        "            total_samples += label.size(0)\n",
        "\n",
        "            # Update progress bar\n",
        "            progress_bar.set_postfix(loss=total_loss / (batch + 1), accuracy=total_correct / total_samples)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} [Validation] - Loss: {total_loss / len(val_loader):.4f}, Accuracy: {total_correct / total_samples:.4f}\") # Use val_loader\n",
        "\n",
        "print(\"\\nTraining finished.\")\n",
        "\n",
        "# Final evaluation on the separate test set after all epochs\n",
        "model.eval()\n",
        "total_loss = 0\n",
        "total_correct = 0\n",
        "total_samples = 0\n",
        "print(\"\\nEvaluating on the full test set...\")\n",
        "with torch.no_grad():\n",
        "    for batch, (image, label) in enumerate(test_loader): # Use test_loader for final evaluation\n",
        "        image, label = image.to(device), label.to(device)\n",
        "        test_logits = model(image)\n",
        "        test_loss = criterion(test_logits, label)\n",
        "        pred_probs = torch.softmax(input=test_logits, dim=1).argmax(dim=1)\n",
        "        correct = (pred_probs == label).sum().item()\n",
        "        total_correct += correct\n",
        "        total_loss += test_loss.item()\n",
        "        total_samples += label.size(0)\n",
        "\n",
        "print(f\"Final Test Loss: {total_loss / len(test_loader):.4f}, Final Test Accuracy: {total_correct / total_samples:.4f}\") # Use test_loader"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VfMIKF0n9NTC",
        "outputId": "873c3b18-3416-433d-cf4a-3f84544f21fb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 345M/345M [00:02<00:00, 124MB/s]\n",
            "100%|██████████| 502/502 [00:00<00:00, 816kB/s]\n",
            "100%|██████████| 15.0k/15.0k [00:00<00:00, 28.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 155MB/s]\n",
            "Epoch 1/50 [Train]: 100%|██████████| 32/32 [00:12<00:00,  2.54it/s, accuracy=0.0402, loss=4.86]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/50 [Train] - Loss: 4.8564, Accuracy: 0.0402\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/50 [Validation]: 100%|██████████| 32/32 [00:06<00:00,  4.73it/s, accuracy=0.0294, loss=13.7]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50 [Validation] - Loss: 13.7219, Accuracy: 0.0294\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/50 [Train]: 100%|██████████| 32/32 [00:10<00:00,  2.95it/s, accuracy=0.0412, loss=4.32]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2/50 [Train] - Loss: 4.3218, Accuracy: 0.0412\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/50 [Validation]: 100%|██████████| 32/32 [00:07<00:00,  4.16it/s, accuracy=0.0912, loss=3.97]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/50 [Validation] - Loss: 3.9745, Accuracy: 0.0912\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/50 [Train]: 100%|██████████| 32/32 [00:10<00:00,  2.98it/s, accuracy=0.0873, loss=3.81]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3/50 [Train] - Loss: 3.8052, Accuracy: 0.0873\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/50 [Validation]: 100%|██████████| 32/32 [00:06<00:00,  4.57it/s, accuracy=0.106, loss=3.92]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/50 [Validation] - Loss: 3.9232, Accuracy: 0.1059\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/50 [Train]: 100%|██████████| 32/32 [00:11<00:00,  2.88it/s, accuracy=0.13, loss=3.55]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4/50 [Train] - Loss: 3.5457, Accuracy: 0.1304\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/50 [Validation]: 100%|██████████| 32/32 [00:06<00:00,  4.75it/s, accuracy=0.146, loss=3.44]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/50 [Validation] - Loss: 3.4391, Accuracy: 0.1461\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/50 [Train]: 100%|██████████| 32/32 [00:11<00:00,  2.87it/s, accuracy=0.152, loss=3.28]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5/50 [Train] - Loss: 3.2842, Accuracy: 0.1520\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/50 [Validation]: 100%|██████████| 32/32 [00:07<00:00,  4.18it/s, accuracy=0.182, loss=3.39]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/50 [Validation] - Loss: 3.3889, Accuracy: 0.1824\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/50 [Train]: 100%|██████████| 32/32 [00:10<00:00,  2.95it/s, accuracy=0.211, loss=3.04]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 6/50 [Train] - Loss: 3.0420, Accuracy: 0.2108\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/50 [Validation]: 100%|██████████| 32/32 [00:06<00:00,  4.85it/s, accuracy=0.25, loss=3.21]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/50 [Validation] - Loss: 3.2140, Accuracy: 0.2500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/50 [Train]: 100%|██████████| 32/32 [00:10<00:00,  2.92it/s, accuracy=0.262, loss=2.82]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 7/50 [Train] - Loss: 2.8207, Accuracy: 0.2618\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/50 [Validation]: 100%|██████████| 32/32 [00:07<00:00,  4.15it/s, accuracy=0.232, loss=3.02]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/50 [Validation] - Loss: 3.0204, Accuracy: 0.2324\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/50 [Train]:   0%|          | 0/32 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-174007864>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \"\"\"\n\u001b[0;32m--> 193\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2818\u001b[0m         )\n\u001b[1;32m   2819\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2820\u001b[0;31m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2822\u001b[0m     return torch.batch_norm(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_verify_batch_size\u001b[0;34m(size)\u001b[0m\n\u001b[1;32m   2781\u001b[0m     \u001b[0;31m#   if reduce(mul, size[2:], size[0]) == 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2782\u001b[0m     \u001b[0msize_prods\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2783\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2784\u001b[0m         \u001b[0msize_prods\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2785\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_prods\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save()"
      ],
      "metadata": {
        "id": "0vxDuM_vHY07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Define the path where you want to save the model\n",
        "model_save_path = \"resnet50_flowers102.pth\" # Use a .pth or .pt extension\n",
        "\n",
        "# Save the model's state dictionary\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "print(f\"Model state dictionary saved to {model_save_path}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "C1Lrvyh1Hhor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# %%\n",
        "# Import necessary libraries\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.transforms import transforms\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image # To open images\n",
        "\n",
        "# %%\n",
        "# Device configuration (must be the same as or compatible with training)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# %%\n",
        "# Define the model architecture (must match the saved model)\n",
        "# Start with a standard ResNet50\n",
        "loaded_model = resnet50(weights=None) # No pre-trained weights here, we'll load our own\n",
        "\n",
        "# Modify the final fully connected layer to match the number of classes trained\n",
        "num_classes = 102 # Must match the number of classes the saved model was trained on\n",
        "loaded_model.fc = nn.Linear(in_features=loaded_model.fc.in_features, out_features=num_classes)\n",
        "\n",
        "# Move the model to the device\n",
        "loaded_model.to(device)\n",
        "\n",
        "# %%\n",
        "# Define the path to the saved model state dictionary\n",
        "model_save_path = \"resnet50_flowers102.pth\" # Replace with the actual path to your saved file\n",
        "\n",
        "# Load the saved state dictionary\n",
        "try:\n",
        "    loaded_model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
        "    print(f\"Model state dictionary loaded successfully from {model_save_path}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Model file not found at {model_save_path}\")\n",
        "    # Handle this error, perhaps exit or skip testing\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model state dictionary: {e}\")\n",
        "    # Handle other loading errors\n",
        "\n",
        "# %%\n",
        "# Set the model to evaluation mode\n",
        "# This is crucial for inference, it disables dropout and batch normalization tracking\n",
        "loaded_model.eval()\n",
        "print(\"Model set to evaluation mode.\")\n",
        "\n",
        "# %%\n",
        "# Prepare a single image for testing\n",
        "\n",
        "# Option 1: Use an image from your dataset (e.g., from the test_ds)\n",
        "# This requires having the dataset loaded again\n",
        "# Assuming test_ds is available or you reload it\n",
        "# Example: Get the first image and label from the test dataset\n",
        "# image, label = test_ds[0]\n",
        "# print(f\"Original label (index): {label}\")\n",
        "\n",
        "# Option 2: Load an external image file\n",
        "# Replace 'path/to/your/image.jpg' with the actual path to an image file\n",
        "try:\n",
        "    image_path = \"./data/flowers-102/test/1/image_06740.jpg\" # Example path, replace with yours\n",
        "    img = Image.open(image_path).convert(\"RGB\") # Open and convert to RGB\n",
        "    print(f\"Loaded image from {image_path}\")\n",
        "\n",
        "    # Apply the same transformations used for testing data during training\n",
        "    # Use the test_transform defined earlier (or redefine it here)\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    image_tensor = test_transform(img).unsqueeze(0) # Add a batch dimension (batch size of 1)\n",
        "    image_tensor = image_tensor.to(device) # Move image to the device\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Image file not found at {image_path}\")\n",
        "    # Handle this error\n",
        "except Exception as e:\n",
        "    print(f\"Error loading or transforming image: {e}\")\n",
        "    # Handle other image processing errors\n",
        "\n",
        "# %%\n",
        "# Perform inference\n",
        "if 'image_tensor' in locals(): # Check if image was loaded successfully\n",
        "    print(\"Performing inference...\")\n",
        "    with torch.no_grad(): # Disable gradient calculation for inference\n",
        "        output = loaded_model(image_tensor)\n",
        "\n",
        "    # Get the predicted class probabilities\n",
        "    probabilities = torch.softmax(output, dim=1)\n",
        "\n",
        "    # Get the predicted class index\n",
        "    predicted_class_index = torch.argmax(probabilities, dim=1).item()\n",
        "\n",
        "    # Optional: If you have a list of class names, you can get the class name\n",
        "    # For Flowers102, the dataset object has .classes or similar\n",
        "    # If you are using Flowers102 dataset object:\n",
        "    # class_names = test_ds.classes\n",
        "    # predicted_class_name = class_names[predicted_class_index]\n",
        "    # print(f\"Predicted class: {predicted_class_name} (Index: {predicted_class_index})\")\n",
        "\n",
        "    # If you don't have class names readily available:\n",
        "    print(f\"Predicted class index: {predicted_class_index}\")\n",
        "\n",
        "    # Optional: Print the probabilities for all classes\n",
        "    # print(\"Class probabilities:\")\n",
        "    # print(probabilities)\n",
        "\n",
        "else:\n",
        "    print(\"Skipping inference due to image loading error.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "3RdNev5bIaNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JzewLPFVJjBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# %%\n",
        "# Assuming the necessary imports, device setup, and loaded_model are already defined\n",
        "\n",
        "# Ensure the model is in evaluation mode\n",
        "loaded_model.eval()\n",
        "print(\"Model set to evaluation mode.\")\n",
        "\n",
        "# %%\n",
        "# Get a single batch from the test loader\n",
        "try:\n",
        "    # Iterate through the first batch of the test loader\n",
        "    # You can stop after getting the first batch\n",
        "    print(\"Getting a batch from the test loader...\")\n",
        "    for batch, (image, label) in enumerate(test_loader):\n",
        "        # We only need the first batch for this example\n",
        "        image_batch = image\n",
        "        label_batch = label\n",
        "        break # Exit the loop after the first batch\n",
        "\n",
        "    print(f\"Obtained a batch of size {image_batch.size(0)} from the test loader.\")\n",
        "\n",
        "    # Select a single image from the batch for inference\n",
        "    # You can change the index (e.g., 0, 1, 2...) to pick a different image from the batch\n",
        "    single_image_tensor = image_batch[0].unsqueeze(0) # Get the first image and add a batch dimension\n",
        "    original_label_for_single_image = label_batch[0].item() # Get the label for that image\n",
        "\n",
        "    # Move the single image tensor to the device\n",
        "    single_image_tensor = single_image_tensor.to(device)\n",
        "\n",
        "    print(f\"Selected image at index 0 from the batch. Original label: {original_label_for_single_image}\")\n",
        "\n",
        "except NameError:\n",
        "    print(\"Error: test_loader not found. Please run the Data Loaders cell first.\")\n",
        "    # Handle this error if test_loader is not available\n",
        "except IndexError:\n",
        "     print(\"Error: The test_loader is empty. Check your dataset and loader setup.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error getting data from test loader: {e}\")\n",
        "    # Handle other errors\n",
        "\n",
        "# %%\n",
        "# Perform inference on the single image\n",
        "if 'single_image_tensor' in locals() and single_image_tensor is not None: # Check if image was successfully obtained\n",
        "    print(\"Performing inference on the single image...\")\n",
        "    with torch.no_grad(): # Disable gradient calculation for inference\n",
        "        output = loaded_model(single_image_tensor)\n",
        "\n",
        "    # Get the predicted class probabilities\n",
        "    probabilities = torch.softmax(output, dim=1)\n",
        "\n",
        "    # Get the predicted class index\n",
        "    predicted_class_index = torch.argmax(probabilities, dim=1).item()\n",
        "\n",
        "    # Optional: If you have a list of class names, you can get the class name\n",
        "    # If you are using Flowers102 dataset object and have access to test_ds:\n",
        "    # class_names = test_ds.classes # Assuming test_ds is available\n",
        "    # predicted_class_name = class_names[predicted_class_index]\n",
        "    # print(f\"Predicted class: {predicted_class_name} (Index: {predicted_class_index})\")\n",
        "\n",
        "    # If you don't have class names readily available:\n",
        "    print(f\"Predicted class index: {predicted_class_index}\")\n",
        "    print(f\"True class index: {original_label_for_single_image}\")\n",
        "\n",
        "    # Optional: Print the top N probabilities\n",
        "    # top_prob, top_cat = torch.topk(probabilities[0], 5) # Get top 5\n",
        "    # print(\"\\nTop 5 Predicted Probabilities and Indices:\")\n",
        "    # for i in range(top_prob.size(0)):\n",
        "    #     print(f\"  Index: {top_cat[i].item()}, Probability: {top_prob[i].item():.4f}\")\n",
        "\n",
        "else:\n",
        "    print(\"Skipping inference as a single image tensor could not be obtained.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "FWzq7OGnJju7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}